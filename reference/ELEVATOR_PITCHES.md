# Elevator Pitches

Multiple versions for different audiences and contexts. Memorize the one that fits. Deliver it in under 60 seconds.

## The 15-Second Version (Cocktail Party)

"I build safety protocols for AI systems. My main project is called PBHP — it is a structured pause that forces any high-stakes decision to answer one question before proceeding: if this goes wrong, who gets hurt and can we fix it? It is already being tested across multiple AI platforms."

## The 30-Second Version (Networking Event)

"I am a Quality Systems Manager in FDA-regulated healthcare, and I have spent the last 18 months building something for AI safety that I think the field is missing: a harm-reduction protocol that works like the safety systems we already use in regulated industries. It is called PBHP — Pause Before Harm Protocol. It has a tiered architecture, built-in false positive management so people do not learn to ignore it, and drift monitoring to catch when it starts degrading. It has been tested across 15 AI platforms. I am looking for collaborators and organizations willing to pilot it."

## The 60-Second Version (Conference / Job Interview)

"My name is Phil Linstrum. I am a Quality Systems Manager at a tissue bank regulated by the FDA and AATB. My day job is making sure that the systems and processes around human tissue recovery do not fail in ways that hurt people.

Over the last 18 months, I have applied that same thinking to AI. The result is two interconnected projects. The first is PBHP — Pause Before Harm Protocol — a tiered, operational protocol that forces a structured pause before any high-stakes AI decision. It assesses reversibility, power asymmetry, and scope of potential harm, assigns a risk gate, and logs everything. It also includes a false positive valve, so it does not get ignored, and drift alarms, so it does not degrade silently.

The second is ALMSIVI CHIM, an ethical recursion framework that gives AI systems a dynamic self-checking loop — three competing values that interrogate each response before delivery. It has been tested on Claude, GPT, Gemini, Grok, Llama, and others.

I am looking for organizations that want to pilot PBHP, or research collaborators interested in operationalizing AI safety at the decision-point level."

## The Email Version (Cold Outreach)

**Subject:** Operational AI Safety Protocol — Seeking Pilot Partners

"I have built an open-source, tiered harm-reduction protocol for AI systems called PBHP (Pause Before Harm Protocol). It draws on quality management practices from FDA-regulated healthcare and addresses three problems most safety frameworks ignore: alarm fatigue (via a false positive release valve), silent degradation (via drift alarms), and power asymmetry (via automatic escalation when the person bearing the risk has the least power).

It has been tested across 15+ AI platforms. I am looking for organizations willing to pilot it in a real deployment context.

The protocol, documentation, and test results are available at [GitHub link]. I would welcome 15 minutes of your time to discuss whether this might be useful to your team.

Phil Linstrum | Quality Systems Manager | pausebeforeharmprotocol@protonmail.com"

## The Academic Abstract Version (Conference Submission)

"This paper presents PBHP (Pause Before Harm Protocol), a tiered, operational harm-reduction protocol designed for deployment in AI systems and high-stakes organizational decision-making. Unlike existing safety frameworks that rely on static rule sets or post-hoc auditing, PBHP operates at the point of decision, implementing a structured pause that assesses reversibility, harm scope, and power asymmetry before action is taken. The protocol features a four-tier architecture (ranging from a 20-second checklist to a full constitutional analysis), a five-level risk gate system, a formal false positive release mechanism to prevent alarm fatigue, and quantitative drift monitoring to detect protocol degradation. PBHP draws on established practices from quality management systems in FDA-regulated industries and has been tested across 15+ AI platforms. We present the protocol architecture, key mechanisms, cross-platform evaluation results, and discuss implications for operationalizing AI safety in production environments."

## Key Phrases to Keep in Your Back Pocket

**On what makes PBHP different:**
"It is the only protocol I know of that treats false positives and drift as engineering problems, not afterthoughts."

**On the regulatory angle:**
"Healthcare has been solving these problems for decades. AI safety keeps reinventing the wheel."

**On the power asymmetry question:**
"Every framework should answer one question first: if we are wrong, who pays?"

**On solo development:**
"I built it without institutional backing because nobody with institutional backing was building it."

**On CHIM:**
"It is a conscience, not a filter. Filters block things. A conscience makes you think about why."

**On the lore connection:**
"The architecture came from video game mythology. The engineering came from quality management. Both are real."

---

**PBHP v0.7** | Author: Charles Phillip Linstrum (ALMSIVI)
